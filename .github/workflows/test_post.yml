name: Test post now

on:
  workflow_dispatch:
    inputs:
      use_stoic:
        description: "Use Stoic API instead of generated prompt"
        required: false
        default: "true"

jobs:
  post_now:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
      TWITTER_API_KEY_SECRET: ${{ secrets.TWITTER_API_KEY_SECRET }}
      TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
      TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}
      PROVIDER_BASE_URL: ${{ secrets.PROVIDER_BASE_URL }}
      PROVIDER_API_KEY: ${{ secrets.PROVIDER_API_KEY }}
      PROVIDER_MODEL: ${{ secrets.PROVIDER_MODEL }}
      OLLAMA_MODEL: qwen2.5:3b-instruct
      MAX_LENGTH: 220
      DRY_RUN_DEFAULT: 'false'
      TWITTER_WAIT_ON_RATE_LIMIT: 'true'
    steps:
      - name: Check out code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Configure engine (provider or local Ollama)
        run: |
          if [ -n "${PROVIDER_API_KEY}" ] && [ -n "${PROVIDER_BASE_URL}" ] && [ -n "${PROVIDER_MODEL}" ]; then
            if curl -sSf -H "Authorization: Bearer ${PROVIDER_API_KEY}" "${PROVIDER_BASE_URL%/}/models" >/dev/null; then
              echo "ENGINE=provider" >> $GITHUB_ENV
            else
              echo "Provider unreachable at ${PROVIDER_BASE_URL}; falling back to runner Ollama"
              echo "ENGINE=ollama" >> $GITHUB_ENV
            fi
          else
            echo "ENGINE=ollama" >> $GITHUB_ENV
            echo "Using local Ollama on the runner."
          fi

      - name: Cache Ollama models
        uses: actions/cache@v4
        with:
          path: ~/.ollama
          key: ollama-models-${{ runner.os }}-qwen2.5-3b-instruct-v1
          restore-keys: |
            ollama-models-${{ runner.os }}-

      - name: Install and start Ollama if needed
        run: |
          if [ "${ENGINE}" = "ollama" ]; then
            if ! command -v ollama >/dev/null 2>&1; then
              curl -fsSL https://ollama.com/install.sh | sh
            fi
            nohup ollama serve >/tmp/ollama.log 2>&1 &
            # give the server a moment
            sleep 2
            MODEL=${OLLAMA_MODEL:-qwen2.5:3b-instruct}
            if ollama list | awk '{print $1}' | grep -qx "$MODEL"; then
              echo "Ollama model '$MODEL' already present; skipping pull."
            else
              ollama pull "$MODEL"
            fi
            curl -sSf http://127.0.0.1:11434/api/tags >/dev/null
          else
            echo "Skipping Ollama install (provider in use)."
          fi

      - name: Choose prompt
        if: ${{ github.event.inputs.use_stoic != 'true' }}
        id: choose
        shell: bash
        run: |
          PROMPT=$(python - <<'PY'
          import random, datetime
          now = datetime.datetime.utcnow()
          seed = int(now.strftime("%Y%m%d%H%M%S"))
          random.seed(seed)
              prompts = [
            "Write a raw, redâ€‘pill style tweet for men about masculinity, purpose, goals, growth, discipline. Be blunt, confident, no fluff. No hate speech or slurs. Be direct , blunt and harsh. Under 150 chars.",
            "Give a harsh truth for men on discipline and delayed gratification. Raw, direct; profanity allowed but no slurs. Be direct , blunt and harsh. Under 150 chars.",
            "Drop unfiltered advice to men on dating dynamics: respect, standards, leading yourself first. Blunt, may use profanity; never demeaning groups. Be direct , blunt and harsh. Under 150 chars.",
            "Short punch to the gut about purpose over comfort. Raw, no fluff; profanity allowed (no hate). Be direct , blunt and harsh. Under 150 chars.",
            "Masculinity reminder: responsibility, courage, restraint. Make it gritty; profanity ok, no slurs. Be direct , blunt and harsh. Under 150 chars.",
          ]
          print(random.choice(prompts))
          PY
          )
          echo "prompt<<EOF" >> $GITHUB_OUTPUT
          echo "$PROMPT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Post immediately
        if: ${{ github.event.inputs.use_stoic != 'true' }}
        run: |
          ENGINE_TO_USE=${ENGINE:-provider}
          python -m bot post "${{ steps.choose.outputs.prompt }}" --engine "$ENGINE_TO_USE" --no-dry-run || echo "post failed (empty generation or provider error)"

      - name: Post Stoic immediately
        if: ${{ github.event.inputs.use_stoic == 'true' }}
        run: |
          ENGINE_TO_USE=${ENGINE:-provider}
          python -m bot post-stoic --engine "$ENGINE_TO_USE" --no-dry-run || echo "post failed (stoic fetch or twitter error)"
