name: Test post now

on:
  workflow_dispatch:

jobs:
  post_now:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
      TWITTER_API_KEY_SECRET: ${{ secrets.TWITTER_API_KEY_SECRET }}
      TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
      TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}
      PROVIDER_BASE_URL: ${{ secrets.PROVIDER_BASE_URL }}
      PROVIDER_API_KEY: ${{ secrets.PROVIDER_API_KEY }}
      PROVIDER_MODEL: ${{ secrets.PROVIDER_MODEL }}
      OLLAMA_MODEL: gemma3:1b
      MAX_LENGTH: 220
      DRY_RUN_DEFAULT: 'false'
    steps:
      - name: Check out code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Configure engine (provider or local Ollama)
        run: |
          if [ -n "${PROVIDER_API_KEY}" ] && [ -n "${PROVIDER_BASE_URL}" ] && [ -n "${PROVIDER_MODEL}" ]; then
            echo "ENGINE=provider" >> $GITHUB_ENV
            # Quick connectivity check for provider
            curl -sSf -H "Authorization: Bearer ${PROVIDER_API_KEY}" "${PROVIDER_BASE_URL%/}/models" >/dev/null || {
              echo "Failed to reach provider at ${PROVIDER_BASE_URL}"; exit 1; }
          else
            echo "ENGINE=ollama" >> $GITHUB_ENV
            echo "Using local Ollama on the runner."
          fi

      - name: Cache Ollama models
        uses: actions/cache@v4
        with:
          path: ~/.ollama
          key: ollama-models-${{ runner.os }}-gemma3-1b-v1
          restore-keys: |
            ollama-models-${{ runner.os }}-

      - name: Install and start Ollama if needed
        run: |
          if [ "${ENGINE}" = "ollama" ]; then
            curl -fsSL https://ollama.com/install.sh | sh
            nohup ollama serve >/tmp/ollama.log 2>&1 &
            # give the server a moment
            sleep 2
            MODEL=${OLLAMA_MODEL:-gemma3:1b}
            ollama pull "$MODEL"
            curl -sSf http://127.0.0.1:11434/api/tags >/dev/null
          else
            echo "Skipping Ollama install (provider in use)."
          fi

      - name: Choose prompt
        id: choose
        shell: bash
        run: |
          PROMPT=$(python - <<'PY'
          import random, datetime
          now = datetime.datetime.utcnow()
          seed = int(now.strftime("%Y%m%d%H%M%S"))
          random.seed(seed)
          prompts = [
            "Drop a random motivational quote, feel free to look up one on the internet or make some shit up. Also be direct a use some vulgar language",
            "Share a direct, no-fluff motivational line about self-discipline, healthy masculinity, and respectful ways to treat women, point out the fact men should keep their options open and be very careful when picking partners. Keep it empowering, never demeaning, no slurs or hate; under 240 chars.",
            "Give a blunt tip on purpose, discipline, or delayed gratification. One line, under 240 chars.",
            "Write a short, gritty reminder about building masculine character: stoicism, responsibility, leading yourself first; always respectful to women. Under 240 chars.",
            "Punchy tweet on habits: consistency over intensity; choose one action today. Under 240 chars."
          ]
          print(random.choice(prompts))
          PY
          )
          echo "prompt<<EOF" >> $GITHUB_OUTPUT
          echo "$PROMPT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Post immediately
        run: |
          ENGINE_TO_USE=${ENGINE:-provider}
          python -m bot post "${{ steps.choose.outputs.prompt }}" --engine "$ENGINE_TO_USE" --no-dry-run || echo "post failed (empty generation or provider error)"
